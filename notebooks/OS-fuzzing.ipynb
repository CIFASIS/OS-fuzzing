{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using Machine Learning to predict the outcome of a fuzzing campaign"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fuzzing campaigns can take time. Unfortunately operating systems contain thousands of different binaries that should be tested and there is no enough time to . Given a very large amount of programs and inputs to fuzz (a.k.a. 'testcases'), in this simple tutorial we are going to see how [VDiscover](http://vdiscover.org/) is used to predict which testcases can be mutated using [zzuf](http://caca.zoy.org/wiki/zzuf) to uncover bugs causing interesting unexpected behaviours like crashes, memory leaks and infinite loops among others. VDiscover is tested on completely new programs and detects with reasonble accuracy if the fuzzer will find a buggy behavior or not.\n",
      "\n",
      "\n",
      "A repository including all the code and the resulting predictor is available [here](https://github.com/CIFASIS/OS-fuzzing)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting started"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before start our fuzzing campaings, it is very important to define a controlled environment to perform our experiments.\n",
      "We use [Vagrant](http://vagrantup.com) for configuring and accesing virtual machines to perform several fuzzing campaings.\n",
      "For our experiments, we started from a [preinstalled Ubuntu image](https://vagrantcloud.com/ubuntu/boxes/trusty32). In particular, we are selected the 32-bit version of Ubuntu since the [x86-64 support of VDiscover is not ready yet](https://github.com/CIFASIS/VDiscover/issues/2). After installing vagrant in the host, an Ubuntu 14.04 virtual machine can be easily fetched executing:\n",
      "\n",
      "    $ vagrant init ubuntu/trusty32\n",
      "$ vagrant up --provider virtualbox\n",
      "\n",
      "Now, our brand-new virtual machine is accessible using ssh:\n",
      "\n",
      "    $ vagrant ssh\n",
      "\n",
      "A full upgrade is recommended before starting, since we want to find bugs in the last version of the programs:\n",
      "\n",
      "    # apt-get update\n",
      "    # apt-get upgrade\n",
      "    \n",
      "Since the software included in the image is rather basic we will probably want to install more packages. \n",
      "Obviously, we will need zzuf to perform the fuzzing (another option is to compile it from its repository). We are also interested in obtain more programs to fuzz focusing on the ones that parse and process files:\n",
      "\n",
      "    # apt-get install zzuf poppler-utils vorbis-tools imagemagick graphviz ..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Discovering bugs automatically"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To discover interesting bugs to analyze, first we need to define where are the executable files to analyze. For example, we can grab binaries to fuzz from the usual directories:\n",
      "\n",
      "    /usr/bin/\n",
      "    /usr/sbin/\n",
      "\n",
      "Also, we need a procedure to extract command-line testcases. Later, using a suitable input file, a random mutation can be used to detect interesting buggy behaviours. The objective of such procedure is to collect a large amount of testcases to train a predictor. Therefore we need an end-to-end fully automatic approach. We utilized several tools and resources:\n",
      "\n",
      "1. **manfuzzer**\n",
      "  \n",
      "  To obtain random command-lines to execute programs we used [manfuzzer](https://github.com/GroundPound/ManFuzzer). This python script produces and executes command-line for fuzzing based on the flags found by running *-h*, *-H*, *--help* and the man page of a program. Our [fork of manfuzzer](https://github.com/CIFASIS/ManFuzzer/) was created to allow the generation of command-line testcases using an input file. After selecting a candidate testcase, it will check if the program tries to open the input file. \n",
      "  <br><br>\n",
      "\n",
      "2. **input files to mutate**\n",
      "  \n",
      "  After we selected a large number of command-line testcases, we mutate different types of input files in order to uncover interesting bugs. Finding a large variety of files to mutate can be a challenging task but fortunately the [fuzzing project](https://fuzzing-project.org/) provides a basic but nice [collection of them](https://files.fuzzing-project.org/).\n",
      "  <!---Obiously, we issues is that we do not know which seed can be suitable for every program. For instance *pdftotext* will likely produce more interesting results if we mutate a pdf file.-->\n",
      "   <br><br> \n",
      "\n",
      "3. **zzuf**: \n",
      "\n",
      "  At the last step of this procedure, we used [zzuf, a popular multi-purpose fuzzer]() to detect crashes, aborts and other interesting unexpected behavior. Since the fuzzing campaings procedure by zzuf are based on random mutation, it can require thousands of tries to uncover an interesting bug (maybe exposed as a crash). Therefore, in order to obtain interesting results we will fuzz up to 100,000 times every testcase mutating between 1% and $50%$ of the input files. If zzuf detects a crash, an abort, a timeout or runs out of memory, the testcase is flagged as **buggy**. \n",
      "    \n",
      "Once we extract a large ammount of testcases to fuzz, we need to run this procedure to collect enough data to train and test VDiscover. In our experiments, more than 64,000 of testcases were selected, but unfortunately, the fuzzing of such a large amount of files will take weeks. Instead of that, a sample of almost 3000 testcases was analyzed to train and test our tool.\n",
      "\n",
      "After several days fuzzing millons of program executions, we collected and analyzed a large number of testcases in our dataset containing the command-line programs and their outcomes after fuzzing it (buggy or robust). The results of the fuzzing campaings in terms of ...\n",
      "\n",
      "<br> <center> <img src=\"files/diagrams/eval_1.svg\"> </center> <br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Predicting interesting testcases before starting a fuzzing campaing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To obtain a prediction of every testcase, we extracted dynamic features using the same procedure and parameters defined in the VDiscover [technical report](http://www.vdiscover.org/report.pdf). This feature extraction requires **only one execution**, the original testcase, which is clearly faster than fuzz it 100000 times. So, if we manage to obtain a reasonable prediction using these features, the \n",
      "\n",
      "Now we are ready to  start learning from them and predict which one we have to fuzz. To perform a sensible evaluation, it is extremly important to separete our dataset in two disjoint subsets: \n",
      "\n",
      "* A set to train the predictor\n",
      "* A set to test the predictor\n",
      "\n",
      "This is a standard procedure in Machine Learning to obtain a sensible measure of the \n",
      "Neverthesless, there is a significant detail that can lead to misleading results because there are several testcases for the same program, and usually zzuf can find a crash in more than one of them. Then VDiscover will easily detect interesting tescases just remembering the features of the buggy program, instead of..\n",
      "\n",
      "In order to test our predictor in a more **realistic** environment, we carefully split the dataset in train and test keeping all the testcases of evey program either in train or in test. No program is selected in the train and test subsets. As a result of this decision, every time we evalute our predictions, we are predicting in never seen binaries.\n",
      "\n",
      "After running 20 independent experiments (e.g. shuffling the training and testing subsets), the average recall scores are:\n",
      "\n",
      "* 92% detecting **robust** testcases\n",
      "* 42% detecting **buggy** testcases \n",
      "\n",
      "As you can see from the results, the predictor is quite effective detecting testcases that uncover no bugs, but not so much with the interesting ones. Despite such inbalance, VDiscover can be very useful to save our time discarding useless testcases from a large set of test. \n",
      "\n",
      "Using VDiscover, we can estimate the reduction in the effort needed to discover new interesting testcases. If we recall the percentage of testcases found interesting (26%) and the rest (74%), we can compute which is the percentage of all the testcases our tool flags as potentially interesting to fuzz using a weighted average:\n",
      "\n",
      "\n",
      "\n",
      "<br> <center> 26% \\* 0.42 + 74% \\* 0.08 = 10.92% + 5.92% = 16.84%</center> <br>\n",
      "\n",
      "which we can visualize here:\n",
      "\n",
      "<br> <center> <img src=\"files/diagrams/eval_2.svg\"> </center> <br>\n",
      "\n",
      "Consequently, by analyzing 16.84% of our test set pointed by VDiscover we can detect 42% of the buggy testcases. \n",
      "As expected, without the help of our tool, a fuzzing campaign will randomly select testcases to mutate. It will need to analyze 42% of the programs to detect 42% of the buggy testcases. Therefore, in terms of our experimental results, we can detect the same amount of vulnerabilities 249% faster ($\\approx$ 42%/16.84%).\n",
      "\n",
      "Finally, we can see an example of how the predictor seems to work. In this experiment, from our dataset, we only selected the wget testcases for testing while the rest of the testcases are used in the training. In the testing, we ..\n",
      "After testing, we summaries the results in the following table:\n",
      "\n",
      "<br> <center> <table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testcase</td><td>predicted</td><td>expected</td></tr><tr><td>/usr/bin/wget:21598</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21558</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21600</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21288</td><td>0</td><td>0</td></tr><tr><td><b>/usr/bin/wget:21329</td><td><b>1</td><td><b>0</td></tr><tr><td>/usr/bin/wget:21442</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21295</td><td>0</td><td>0</td></tr><tr><td>/usr/bin/wget:21512</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21515</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21552</td><td>1</td><td>1</td></tr><tr><td>/usr/bin/wget:21584</td><td>1</td><td>1</td></tr></table> </center>\n",
      "\n",
      "VDiscover only misses one testcases despite it never saw wget."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Interesting bugs (re)discovered during these experiments"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A format string vulnerability in graphviz, very similar to [CVE-2014-9157](https://security-tracker.debian.org/tracker/CVE-2014-9157) [reported]\n",
      "* [An out-of-bound read using invalid UNICODE strings in libidn](https://bugzilla.redhat.com/show_bug.cgi?id=1197796) (affecting wget at least)\n",
      "* [A few crashes in oggenc](https://bugzilla.redhat.com/show_bug.cgi?id=1185272)\n",
      "* ..."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}